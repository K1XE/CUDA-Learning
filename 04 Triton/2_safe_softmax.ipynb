{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def safeSoftmaxKernel(\n",
    "    outs, ins,\n",
    "    outs_stride, ins_stride,\n",
    "    cols, BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    bid = tl.program_id(axis=0)\n",
    "    ins_start = ins + bid * ins_stride\n",
    "    outs_start = outs + bid * outs_stride\n",
    "    bsz = tl.load(ins_start + tl.arange(0, BLOCK_SIZE), \n",
    "                  mask=tl.arange(0, BLOCK_SIZE) < cols, \n",
    "                  other=float('-inf'))\n",
    "    maxV = tl.max(bsz, axis=0)\n",
    "    upper = tl.exp(bsz - maxV)\n",
    "    sums = tl.sum(upper, axis=0)\n",
    "    softmax_output = upper / sums\n",
    "    tl.store(outs_start + tl.arange(0, BLOCK_SIZE), \n",
    "             softmax_output, \n",
    "             mask=tl.arange(0, BLOCK_SIZE) < cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_safe_softmax(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "    output = torch.empty_like(x)\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "    BLOCK_SIZE = min(BLOCK_SIZE, 1024)\n",
    "    \n",
    "    grid = (n_rows,)\n",
    "\n",
    "    safeSoftmaxKernel[grid](\n",
    "        output,\n",
    "        x,\n",
    "        output.stride(0),\n",
    "        x.stride(0), \n",
    "        n_cols, \n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "    )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(256, 1024, device='cuda')\n",
    "\n",
    "# 3.1 PyTorch 自带 softmax\n",
    "torch_result = torch.softmax(x, dim=1)\n",
    "\n",
    "# 3.2 Triton 版 softmax\n",
    "triton_result = triton_safe_softmax(x)\n",
    "\n",
    "# 3.3 计算最大误差\n",
    "max_diff = torch.max(torch.abs(torch_result - triton_result))\n",
    "print(f\"pytorch与triton的safe-softmax最大误差为: {max_diff:.2e}\")\n",
    "\n",
    "# 3.4 检查是否在给定精度范围内\n",
    "is_close = torch.allclose(torch_result, triton_result, rtol=1e-5, atol=1e-5)\n",
    "print(f\"结果正确否: {is_close}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
